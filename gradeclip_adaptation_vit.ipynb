{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e3ac5-d83b-480f-b2d1-9f5410aa3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "_transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(image_processor.image_mean, image_processor.image_std),\n",
    "    ])\n",
    "\n",
    "def imgprocess_keepsize(img, patch_size=[16, 16], scale_factor=1):\n",
    "    w, h = img.size\n",
    "    ph, pw = patch_size\n",
    "    nw = int(w * scale_factor / pw + 0.5) * pw\n",
    "    nh = int(h * scale_factor / ph + 0.5) * ph\n",
    "\n",
    "    ResizeOp = Resize((nh, nw), interpolation=InterpolationMode.BICUBIC)\n",
    "    img = ResizeOp(img).convert(\"RGB\")\n",
    "    return _transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf5f03-ec88-49db-8d25-638200f08ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_attention(attention_layer, x, head_mask=None, output_attentions=False):\n",
    "    num_attention_heads = 1\n",
    "    attention_head_size = 768 // num_attention_heads\n",
    "    \n",
    "    def transpose_for_scores(x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "        \n",
    "    mixed_query_layer = attention_layer.query(x)\n",
    "\n",
    "    key_layer = transpose_for_scores(attention_layer.key(x))\n",
    "    value_layer = transpose_for_scores(attention_layer.value(x))\n",
    "    query_layer = transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "    attention_scores = attention_scores / math.sqrt(attention_layer.attention_head_size)\n",
    "\n",
    "    # Normalize the attention scores to probabilities.\n",
    "    attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    # This is actually dropping out entire tokens to attend to, which might\n",
    "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "    attention_probs = attention_layer.dropout(attention_probs)\n",
    "\n",
    "    # Mask heads if we want to\n",
    "    if head_mask is not None:\n",
    "        attention_probs = attention_probs * head_mask\n",
    "\n",
    "    context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "    new_context_layer_shape = context_layer.size()[:-2] + (attention_layer.all_head_size,)\n",
    "    context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "    outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "    return outputs, (query_layer, key_layer, value_layer)\n",
    "\n",
    "\n",
    "def vit_classifier(x):\n",
    "    # modified from ViT\n",
    "    batch_size, num_channels, height, width = x.shape\n",
    "    print(height, width)\n",
    "\n",
    "    ## keep image size \n",
    "    x = model.vit.embeddings(x, interpolate_pos_encoding=True)\n",
    "\n",
    "    for i, layer_module in enumerate(model.vit.encoder.layer[:-1]):\n",
    "        layer_outputs = layer_module(x, None, False)\n",
    "        x = layer_outputs[0]\n",
    "    \n",
    "\n",
    "    ### the last layer in ViT\n",
    "    lastLY = model.vit.encoder.layer[-1]\n",
    "\n",
    "    x = lastLY.layernorm_before(x)\n",
    "\n",
    "    attention_layer = lastLY.attention.attention\n",
    "    self_outputs, (q,k,v) = forward_attention(attention_layer, x)\n",
    "    attention_output = lastLY.attention.output(self_outputs[0], x)\n",
    "\n",
    "    # residual connection\n",
    "    x = attention_output + x\n",
    "\n",
    "    # in ViT, layernorm is also applied after self-attention\n",
    "    layer_output = lastLY.layernorm_after(x)\n",
    "    layer_output = lastLY.intermediate(layer_output)\n",
    "\n",
    "    # second residual connection is done here\n",
    "    layer_output = lastLY.output(layer_output, x)\n",
    "    sequence_output = model.vit.layernorm(layer_output)\n",
    "\n",
    "    logits = model.classifier(sequence_output[:, 0, :])\n",
    "    return logits, self_outputs[0], (q.detach(), k.detach(), v.detach()), (int(height//16), int(width//16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa7449-ab54-4b63-b5b4-1839d4325996",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './whippet.png'\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "inputs = image_processor(img, return_tensors=\"pt\")['pixel_values']\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.to(device)).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8460d9f-fddd-4ea4-b2d4-0e8b1a6f87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_eclip(c, q, k, v, att_output, map_size, withksim=True):\n",
    "    D = k.shape[-1]\n",
    "    ## gradient on last attention output\n",
    "    grad = torch.autograd.grad(\n",
    "        c,\n",
    "        att_output,\n",
    "        retain_graph=True)[0]\n",
    "    grad = grad.detach()\n",
    "    grad_cls = grad[0,:1,:]\n",
    "    if withksim:\n",
    "        q_cls = q[0,0,:1,:]\n",
    "        k_patch = k[0,0,1:,:]\n",
    "        q_cls = F.normalize(q_cls, dim=-1)\n",
    "        k_patch = F.normalize(k_patch, dim=-1)\n",
    "        cosine_qk = (q_cls * k_patch).sum(-1) \n",
    "        cosine_qk = (cosine_qk-cosine_qk.min()) / (cosine_qk.max()-cosine_qk.min())\n",
    "        emap_lastv = F.relu_((grad_cls * v[0,0,1:,:] * cosine_qk[:,None]).detach().sum(-1)) # \n",
    "    else:\n",
    "        emap_lastv = F.relu_((grad_cls * v[0,0,1:,:]).detach().sum(-1)) \n",
    "    return emap_lastv.reshape(*map_size)\n",
    "\n",
    "def visualize(hmap, raw_image, resize):\n",
    "    hmap -= hmap.min()\n",
    "    hmap /= hmap.max()\n",
    "    image = np.asarray(raw_image.copy())\n",
    "    hmap = resize(hmap.unsqueeze(0))[0].cpu().numpy()\n",
    "    color = cv2.applyColorMap((hmap*255).astype(np.uint8), cv2.COLORMAP_JET) # cv2 to plt\n",
    "    color = cv2.cvtColor(color, cv2.COLOR_BGR2RGB)\n",
    "    c_ret = np.clip(image * (1 - 0.5) + color * 0.5, 0, 255).astype(np.uint8)\n",
    "    return c_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707ac8a-27b3-427f-9b4e-510417e78ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = imgprocess_keepsize(img)\n",
    "print(inputs.shape)\n",
    "logits, last_att_outputs, (q, k, v), map_size = vit_classifier(inputs.unsqueeze(0).to(device))\n",
    "print(logits.shape, logits.requires_grad, last_att_outputs.requires_grad, last_att_outputs.shape, q.shape, k.shape, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c232cf-7dca-4e5a-9e09-315569afb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "emap = grad_eclip(logits[:,predicted_label], q, k, v, last_att_outputs, map_size, withksim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c76c7-4657-43de-b1ee-ad7ce55251f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = img.size\n",
    "print(w,h)\n",
    "resize = Resize((h,w))\n",
    "\n",
    "c_ret = visualize(emap, img.copy(), resize)\n",
    "plt.imshow(c_ret)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ba2a5-706e-4572-8d5d-4d864f12bc20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
